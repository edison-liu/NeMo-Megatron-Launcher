{{ $config := .Values.trainingConfig }}

apiVersion: kubeflow.org/v1
kind: PyTorchJob
metadata:
  name: rlhf-ppo
  labels:
    app: rlhf-ppo
spec:
  pytorchReplicaSpecs:
    Critic:
      replicas: {{ .Values.image.nodes }}
      template:
        spec:
          containers:
          - name: critic
            image: {{ .Values.image.trainingImage }}
            env:
              {{- range $key, $value := $config.envVars }}
              - name: {{ $key }}
                value: {{ $value | quote }}
              {{- end}}
            {{ if eq $config.wandbKey "nil" }}
            command: ["torchrun"]
            args:
              - "--nnodes={{ .Values.image.nodes }}"
              - "--rdzv-backend=c10d"
              - "--rdzv-endpoint=rlhf-ppo-critic-0"
              - "--nproc_per_node={{ .Values.image.numGPUs }}"
              - "/opt/NeMo-Aligner/examples/nlp/nlp/gpt/serve_ppo_critic.py"
              - "--config-path=/config"
              - "--config-name=config.yaml"
              - "trainer.ppo.port=5567"
            {{ else }}
            command: ["bash", "-c"]
            args:
              - "wandb login {{ $config.wandbKey }} && torchrun --nnodes={{ .Values.image.nodes }} --rdzv-backend=c10d --rdzv-endpoint=rlhf-ppo-critic-0 --nproc_per_node={{ .Values.image.numGPUs }} /opt/NeMo-Aligner/examples/nlp/nlp/gpt/serve_ppo_critic.py --config-path=/config --config-name=config.yaml"
            {{ end }}
            imagePullPolicy: Always
            securityContext:
              capabilities:
                add: [ "IPC_LOCK" ]
            resources:
              requests:
                nvidia.com/gpu: {{ .Values.image.numGPUs }}
                {{ $config.ibResourceName }}: {{ $config.ibCount }}
              limits:
                nvidia.com/gpu: {{ .Values.image.numGPUs }}
                {{ $config.ibResourceName }}: {{ $config.ibCount }}
            volumeMounts:
            - mountPath: {{ $config.NFSPath }}
              name: workspace
            - mountPath: /dev/shm
              name: dshm
            - mountPath: /config
              name: rlhf-ppo-config
          restartPolicy: Never
          imagePullSecrets:
          - name: {{ .Values.image.pullSecret }}
          volumes:
          - name: workspace
            nfs:
              server: {{ $config.NFSServer }}
              path: {{ $config.NFSPath }}
          - name: dshm
            emptyDir:
              medium: Memory
              sizeLimit: {{ $config.shmSize }}
          - configMap:
              name: rlhf-ppo-config
            name: rlhf-ppo-config
          {{ if ne $config.dnsPolicy "nil" }}
          dnsPolicy: {{ $config.dnsPolicy }}
          {{ end }}

    Actor:
      replicas: {{ .Values.image.nodes }}
      template:
        spec:
          containers:
          - name: pytorch
            image: {{ .Values.image.trainingImage }}
            env:
              {{- range $key, $value := $config.envVars }}
              - name: {{ $key }}
                value: {{ $value | quote }}
              {{- end}}
            {{ if eq $config.wandbKey "nil" }}
            command: ["torchrun"]
            args:
              - "--nnodes={{ .Values.image.nodes }}"
              - "--rdzv-backend=c10d"
              - "--rdzv-endpoint=rlhf-ppo-critic-0"
              - "--nproc_per_node={{ .Values.image.numGPUs }}"
              - "/opt/NeMo-Aligner/examples/nlp/nlp/gpt/train_gpt_ppo_actor.py"
              - "--config-path=/config"
              - "--config-name=config.yaml"
              - "remote_critic_rm.critic.ip=rlhf-ppo.default.svc.cluster.local"
              - "remote_critic_rm.critic.port=5567"
            {{ else }}
            command: ["bash", "-c"]
            args:
              - "wandb login {{ $config.wandbKey }} && torchrun --nnodes={{ .Values.image.nodes }} --rdzv-backend=c10d --rdzv-endpoint=rlhf-ppo-critic-0 --nproc_per_node={{ .Values.image.numGPUs }} /opt/NeMo-Aligner/examples/nlp/nlp/gpt/train_gpt_ppo_actor.py --config-path=/config --config-name=config.yaml"
            {{ end }}
            imagePullPolicy: Always
            securityContext:
              capabilities:
                add: [ "IPC_LOCK" ]
            resources:
              requests:
                nvidia.com/gpu: {{ .Values.image.numGPUs }}
                {{ $config.ibResourceName }}: {{ $config.ibCount }}
              limits:
                nvidia.com/gpu: {{ .Values.image.numGPUs }}
                {{ $config.ibResourceName }}: {{ $config.ibCount }}
            volumeMounts:
            - mountPath: {{ $config.NFSPath }}
              name: workspace
            - mountPath: /dev/shm
              name: dshm
            - mountPath: /config
              name: rlhf-ppo-config
          restartPolicy: Never
          imagePullSecrets:
          - name: {{ .Values.image.pullSecret }}
          volumes:
          - name: workspace
            nfs:
              server: {{ $config.NFSServer }}
              path: {{ $config.NFSPath }}
          - name: dshm
            emptyDir:
              medium: Memory
              sizeLimit: {{ $config.shmSize }}
          - configMap:
              name: rlhf-ppo
            name: rlhf-ppo-config
          {{ if ne $config.dnsPolicy "nil" }}
          dnsPolicy: {{ $config.dnsPolicy }}
          {{ end }}
